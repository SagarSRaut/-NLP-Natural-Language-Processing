{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b29bbe-7fae-4bcd-a394-1e87dcc9c9e0",
   "metadata": {},
   "source": [
    "### Library installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e161c1a1-ffa5-4dba-aab5-8e8413d514d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7f11e-fc64-4da3-8225-cf990769c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698506ec-eb49-4c2b-93a3-76993e925002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # tokenization\n",
    "nltk.download('stopwords') # stopwords removal\n",
    "nltk.download('averaged_perceptron_tagger') # POS (part of speech) tagging\n",
    "nltk.download('wordnet') # wordnet database and lemmatization\n",
    "nltk.download('omw-1.4')  #stemming\n",
    "nltk.download('indian') # Indian language POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1bc3f0-98c8-4e3f-acbb-5a75f7ccaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') #chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37527059-42fc-4a64-aa9b-d48a00e789c0",
   "metadata": {},
   "source": [
    "#### Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9f49e9-0386-41b7-96f3-3e0d029e2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectuvely.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b56f38-acfb-4fed-a9c2-829d2b1d5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average of ages mentioned in the above mentioned sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4971162e-0372-495a-a633-4ac3a637867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(sent)):\n",
    "#     if sent.isdigit():\n",
    "#         print(sent.find[i])\n",
    "\n",
    "lst = []\n",
    "lst1 = []\n",
    "\n",
    "lst = sent.split(' ')\n",
    "for i in lst:\n",
    "    if i.isnumeric():\n",
    "        lst1.append(i)\n",
    "print(sum([int(i) for i in lst1])/len(lst1))\n",
    "# print(sum(int(lst1))/len(lst1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9384c1e9-095d-4317-a461-e041e6603251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73eecc6a-dc8a-436b-899f-d310a4492f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages=[]\n",
    "ages=[int(word) for word in sent.split() if word.isdigit()]\n",
    "sum(ages)/len(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5f5dd-6fc9-4149-a88d-ce6b9f39eb6f",
   "metadata": {},
   "source": [
    "### Tokeniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e5b0b9-df86-48b0-adb5-e4c9797acf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe310f0-9f8e-42a9-9e9f-4941d854ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76979ae9-724c-4c0a-84b8-311124a160a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d356d666-fb0c-4d62-a21f-edcb810550b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8b6225-942c-4fd3-9956-c0745bcb62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of punctuation symbbols present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a774bc-4624-4fff-8747-0c0d34b74a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# l1 = []\n",
    "# for i in p_punc:\n",
    "#     if i.isalnum():\n",
    "#         l.append(i)\n",
    "#     else:\n",
    "#         l1.append(i)\n",
    "\n",
    "# print(len([int(i) for i in l1])/(sum([int(i) for i in l])+sum([int(i) for i in l1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176ad7ba-026a-4830-b2fe-562174be4cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_count = len([word for word in word_tokenize(sent) if not word.isalnum()])\n",
    "punct_count / len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90d657a-6556-48ca-ba86-f333eb789b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11fa7ab1-a94b-43ac-afad-4d1775183072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59ebd57d-0125-4a8a-ab57-78969d56148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42785d0-1676-4c57-b6d4-cb51b3059d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcdef'\n",
    "sys.getsizeof(char) #50 + 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bc9d0c-2d33-4578-b4a7-65d3c722eb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ऩ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a0fcf0-f13e-4f21-8a6b-0d5cf126a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'वी'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0935\\u0940'  #unicode\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d1104e-02e6-4e6b-93af-e4a4b1b90a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'व'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7abd2507-5c9a-489c-b706-9d7c5b7086c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('वी')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848f12a9-eb9c-4191-8afe-35333363143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('😂')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9464afa-1426-4490-a943-5d6112271fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'अमेय पणशीकर'\n",
    "sys.getsizeof(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c36ad916-189e-4cc0-9afe-f29ee3ead314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('अ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3ba391-a219-4df2-a511-d468ba68c092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[-2].endswith('क')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "116de66e-b865-4f84-b5b5-8b13e8b3665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('ण')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db3ec18-0a36-414d-9486-4e26b6516c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0587314-573b-4fa8-934f-6364510a1838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['रोहित', 'रोवमन']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['विराट', 'रोहित' ,'चहल', 'बुमराह ','रोवमन']\n",
    "[i for i in name if i.startswith('रो')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "351ab3b7-75f3-41e7-b5ee-02ae01054594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = '४८ कि. मी. अंतरावर आणि पुणे जिल्ह्यातील वेल्हे तालुक्यात व भोर गावाच्या वायव्येला २४ कि.मी. अंतरावर नीरा-वेळवंडी-कानंदी आणि गुंजवणी या नद्यांच्या खोऱ्यांच्या बेचक्यात मुरुंबदेवाचा डोंगर उभा आहे. मावळ भागामध्ये राज्यविस्तार साध्य करण्यासाठी राजगड आणि तोरणा हे दोन्ही किल्ले मोक्याच्या ठिकाणी होते. तोरणा Archived 2020-09-20 at the Wayback Machine. किल्ल्याचा बालेकिल्ला आकाराने लहान असल्यामुळे राजकीय केंद्र म्हणून हा किल्ला सोयीचा नव्हता. त्यामानाने राजगड दुर्गम असून त्याचा बालेकिल्ला बराच मोठा आहे. शिवाय राजगडाकडे कोणत्याही बाजूने येताना एखादी टेकडी किंवा नदी ओलांडावीच लागते. एवढी सुरक्षितता होती,म्हणून आपले राजकीय केंद्र म्हणून शिवाजी महाराजांनी'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30986bc7-88ac-42dd-a1da-aaec08ea8192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['४८', 'कि', '.', 'मी', '.', 'अंतरावर', 'आणि', 'पुणे', 'जिल्ह्यातील', 'वेल्हे']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ded94-9dea-41d2-bc6e-64b7014f83c4",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffa00c42-9e66-4b40-8ede-cde4433c92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!\t How are you?\n",
      "Welcome to the world of\t Python Programming.\n"
     ]
    }
   ],
   "source": [
    "f = open('mydata.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "#read ==> reads the complete file and stores it as a string\n",
    "#readable ==> returns true or false based on whether the file is readable or not\n",
    "#readline ==> returns a single line as a string\n",
    "#readlines ==> returns multiple lines and returns a list of strings (one element = one line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06ddf50-0820-4005-b3f8-499da502f7d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!\\t',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of\\t',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "#create object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706b64e-3803-46ab-a454-c38e9299dc16",
   "metadata": {},
   "source": [
    "  ### Tab tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c4edca9-624e-4805-9bd8-43bf9abc4ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!',\n",
       " ' How are you?\\nWelcome to the world of',\n",
       " ' Python Programming.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "#create object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741703e-521a-406c-900f-b6b793ea659a",
   "metadata": {},
   "source": [
    "### LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "208a65a4-33cb-45f1-abaa-e3ca612f76e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!\\t How are you?',\n",
       " 'Welcome to the world of\\t Python Programming.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "#create object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce555d2-44a4-4301-979e-fe41180ad94a",
   "metadata": {},
   "source": [
    "### White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f72e86-b981-4658-8afc-f328d84d3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import WhitespaceTokenizer  #exactly similar to the split method\n",
    "\n",
    "#create object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943e565-cee7-4e10-9afa-99d82dfc90a0",
   "metadata": {},
   "source": [
    "### MWE (Multi word tokenizer) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5abf13-7494-40fb-9a9b-6d36852b55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '''The Van Rossum is Python creator, visiting Pune this week. \n",
    "The development community is very eager to meet Van Rossum.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "478f7494-97d1-4cd6-98e1-7367f7a62614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Van Rossum is Python creator, visiting Pune this week. \\nThe development community is very eager to meet Van Rossum.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31a95fa7-58e9-44e4-b784-ff86b3acc55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce7f21-38c0-488a-a832-075fcf9cca6f",
   "metadata": {},
   "source": [
    "### Tweet  Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00058cb1-7ec9-456e-af44-0fa3a166c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " ':D',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " ':',\n",
       " 'C',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "sent2='Hello Friends :)!How are you? Welcome to the world of :D\tPython Programming :C.'\n",
    "#create object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    " \n",
    "# tokenize the data\n",
    "tk.tokenize(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7517024-75c3-44c9-9021-8f137b52841e",
   "metadata": {},
   "source": [
    "### Emoji Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fc28062-81c9-47e7-8d9e-8d73df132b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_emoji = '💯Hello Friends! welcome 🙏dog🐶 😂😂 sagar raut ha amcha maanus ahe to java cha topper ahe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f22310d-9555-4cba-962e-388a9f3be34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'💯Hello Friends! welcome 🙏dog🐶 😂😂 sagar raut ha amcha maanus ahe to java cha topper ahe'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61ab53e8-8a96-4221-b78b-e1074dc49d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['💯',\n",
       " 'Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'welcome',\n",
       " '🙏',\n",
       " 'dog',\n",
       " '🐶',\n",
       " '😂',\n",
       " '😂',\n",
       " 'sagar',\n",
       " 'raut',\n",
       " 'ha',\n",
       " 'amcha',\n",
       " 'maanus',\n",
       " 'ahe',\n",
       " 'to',\n",
       " 'java',\n",
       " 'cha',\n",
       " 'topper',\n",
       " 'ahe']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer \n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(sent_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594da126-65e1-4328-80d2-0b612adf1510",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaf83846-d011-45d1-880b-20777629d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "print('Tokens: ')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fae197db-84b8-492e-a805-94b64a32e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c5731f1-1ecb-4f96-aa38-3d08a1dd5e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "546f3885-2772-41ae-85d3-75388e99767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = data.split('\\n')[1:]\n",
    "# def custom_tokenizer(out):\n",
    "#     return re.split().replace('\\t',' ')\n",
    "    \n",
    "# tokens = custom_tokenizer(str(out))\n",
    "# print('Tokens: ')\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69f2d798-ea0d-4a53-bc7c-f54d675112af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = []\n",
    "\n",
    "for x in data.split('\\n'):\n",
    "    inner_list  = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    newdata.append(inner_list)\n",
    "newdata[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8486d-a372-493c-88a8-1e1fb9cdb9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
